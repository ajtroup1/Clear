# 01.clr
Welcome to Clear

*This file is a log of all activity that occured during the interpretation of your source code.*

## Lexical Analysis / Lexing / Tokenization
Lexing (or tokenization) is the process of converting a sequence of characters into a sequence of tokens.

These tokens are the simplest level of strutured data pertaining to the source code information.

*Example Token*: `let` (Type: LET, Literal: 'let')

- Optionally, the token can track other information such as line and column information, which is used for error reporting.

	- Token: `return` (Type: RETURN, Literal:'return', Line: 6, Column: 12)

The lexer reads the source code character by character and generates tokens based on the characters it reads. The lexer is also the first step in the compilation or interpretation process. The lexer is additionally responsible for removing whitespace and comments from the source code.

**Source code:** 
```js

let x = 7;

x = x + 10 + 20;

return x + 3;

```

Initializing lexer...

**Lexing source code...**

### Live encounters:

1. Encountered newline at [line: 1, col: 1]
1. Tokenized Token::LET 'let' at [line: 2, col: 1]
1. Tokenized Token::IDENT 'x' at [line: 2, col: 5]
1. Tokenized Token::= '=' at [line: 2, col: 7]
1. Tokenized Token::INT '7' at [line: 2, col: 9]
1. Tokenized Token::; ';' at [line: 2, col: 10]
1. Encountered newline at [line: 2, col: 11]
1. Encountered newline at [line: 3, col: 1]
1. Tokenized Token::IDENT 'x' at [line: 4, col: 1]
1. Tokenized Token::= '=' at [line: 4, col: 3]
1. Tokenized Token::IDENT 'x' at [line: 4, col: 5]
1. Tokenized Token::+ '+' at [line: 4, col: 7]
1. Tokenized Token::INT '10' at [line: 4, col: 9]
1. Tokenized Token::+ '+' at [line: 4, col: 12]
1. Tokenized Token::INT '20' at [line: 4, col: 14]
1. Tokenized Token::; ';' at [line: 4, col: 16]
1. Encountered newline at [line: 4, col: 17]
1. Encountered newline at [line: 5, col: 1]
1. Tokenized Token::RETURN 'return' at [line: 6, col: 1]
1. Tokenized Token::IDENT 'x' at [line: 6, col: 8]
1. Tokenized Token::+ '+' at [line: 6, col: 10]
1. Tokenized Token::INT '3' at [line: 6, col: 12]
1. Tokenized Token::; ';' at [line: 6, col: 13]
1. Encountered newline at [line: 6, col: 14]
1. Tokenized Token::EOF '' at [line: 0, col: 0]
1. Tokenized Token::EOF '' at [line: 0, col: 0]


**Here is the stream of all tokens generated by the lexer:**

```
	- Token::EOF '' at [line: 0, col: 0]
	- Token::EOF '' at [line: 0, col: 0]
	- Token::LET 'let' at [line: 2, col: 1]
	- Token::IDENT 'x' at [line: 2, col: 5]
	- Token::= '=' at [line: 2, col: 7]
	- Token::INT '7' at [line: 2, col: 9]
	- Token::; ';' at [line: 2, col: 10]
	- Token::IDENT 'x' at [line: 4, col: 1]
	- Token::= '=' at [line: 4, col: 3]
	- Token::IDENT 'x' at [line: 4, col: 5]
	- Token::+ '+' at [line: 4, col: 7]
	- Token::INT '10' at [line: 4, col: 9]
	- Token::+ '+' at [line: 4, col: 12]
	- Token::INT '20' at [line: 4, col: 14]
	- Token::; ';' at [line: 4, col: 16]
	- Token::RETURN 'return' at [line: 6, col: 1]
	- Token::IDENT 'x' at [line: 6, col: 8]
	- Token::+ '+' at [line: 6, col: 10]
	- Token::INT '3' at [line: 6, col: 12]
	- Token::; ';' at [line: 6, col: 13]
```
## Parsing
parsing description here

Successfully parsed the program!

Here is your program node in tree format:
```json
&ast.Program{
  NoStatements: false,
  Statements: []ast.Statement{
    &ast.LetStatement{
      Token: token.Token{
        Type: "LET",
        Literal: "let",
        Line: 2,
        Col: 1,
      },
      Name: &ast.Identifier{
        Token: token.Token{
          Type: "IDENT",
          Literal: "x",
          Line: 2,
          Col: 5,
        },
        Value: "x",
      },
      Value: &ast.IntegerLiteral{
        Token: token.Token{
          Type: "INT",
          Literal: "7",
          Line: 2,
          Col: 9,
        },
        Value: 7,
      },
    },
    &ast.AssignStatement{
      Token: token.Token{
        Type: "IDENT",
        Literal: "x",
        Line: 4,
        Col: 1,
      },
      Name: &ast.Identifier{
        Token: token.Token{
          Type: "IDENT",
          Literal: "x",
          Line: 4,
          Col: 1,
        },
        Value: "x",
      },
      Value: &ast.InfixExpression{
        Token: token.Token{
          Type: "+",
          Literal: "+",
          Line: 4,
          Col: 12,
        },
        Left: &ast.InfixExpression{
          Token: token.Token{
            Type: "+",
            Literal: "+",
            Line: 4,
            Col: 7,
          },
          Left: &ast.Identifier{
            Token: token.Token{
              Type: "IDENT",
              Literal: "x",
              Line: 4,
              Col: 5,
            },
            Value: "x",
          },
          Operator: "+",
          Right: &ast.IntegerLiteral{
            Token: token.Token{
              Type: "INT",
              Literal: "10",
              Line: 4,
              Col: 9,
            },
            Value: 10,
          },
        },
        Operator: "+",
        Right: &ast.IntegerLiteral{
          Token: token.Token{
            Type: "INT",
            Literal: "20",
            Line: 4,
            Col: 14,
          },
          Value: 20,
        },
      },
    },
    &ast.ReturnStatement{
      Token: token.Token{
        Type: "RETURN",
        Literal: "return",
        Line: 6,
        Col: 1,
      },
      ReturnValue: &ast.InfixExpression{
        Token: token.Token{
          Type: "+",
          Literal: "+",
          Line: 6,
          Col: 10,
        },
        Left: &ast.Identifier{
          Token: token.Token{
            Type: "IDENT",
            Literal: "x",
            Line: 6,
            Col: 8,
          },
          Value: "x",
        },
        Operator: "+",
        Right: &ast.IntegerLiteral{
          Token: token.Token{
            Type: "INT",
            Literal: "3",
            Line: 6,
            Col: 12,
          },
          Value: 3,
        },
      },
    },
  },
  Modules: nil,
}
```

## Evaluation
evaluation description here

